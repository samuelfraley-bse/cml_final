{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEF Ensemble Model Comparison\n",
    "## Goal: Find best model for Kaggle submission + causal inference insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup paths\n",
    "BASE_DIR = Path.cwd().parents[1]\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "sys.path.append(str(SRC_DIR))\n",
    "sys.path.append(\"/home/claude\")  # for the ensemble module\n",
    "\n",
    "from hef_prep import prepare_data\n",
    "from hef_ensemble_models import HEFEnsembleModels\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Try both with and without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data WITH feature engineering\n",
    "X_fe, y, X_test_fe = prepare_data(\n",
    "    task=\"class\",\n",
    "    leak_cols=[\n",
    "        \"ADMITTIME\", \"ICD9_diagnosis\", \"DIAGNOSIS\", \n",
    "        \"DOB\", \"DEATHTIME\", \"DISCHTIME\", \"DOD\", \n",
    "        \"LOS\", \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    ],\n",
    "    apply_fe=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data with Feature Engineering\")\n",
    "print(\"=\"*60)\n",
    "print(f\"X shape: {X_fe.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Positive rate: {y.mean():.3f}\")\n",
    "print(f\"X_test shape: {X_test_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data WITHOUT feature engineering (for comparison)\n",
    "X_raw, y_raw, X_test_raw = prepare_data(\n",
    "    task=\"class\",\n",
    "    leak_cols=[\n",
    "        \"ADMITTIME\", \"ICD9_diagnosis\", \"DIAGNOSIS\", \n",
    "        \"DOB\", \"DEATHTIME\", \"DISCHTIME\", \"DOD\", \n",
    "        \"LOS\", \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    ],\n",
    "    apply_fe=False,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data WITHOUT Feature Engineering\")\n",
    "print(\"=\"*60)\n",
    "print(f\"X shape: {X_raw.shape}\")\n",
    "print(f\"X_test shape: {X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the FE version for main analysis\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_fe, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Valid: {X_valid.shape[0]} samples\")\n",
    "print(f\"Train positive rate: {y_train.mean():.3f}\")\n",
    "print(f\"Valid positive rate: {y_valid.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric columns: {len(num_cols)}\")\n",
    "print(f\"Categorical columns: {len(cat_cols)}\")\n",
    "\n",
    "# Numeric pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Multiple Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ensemble model collection\n",
    "ensemble = HEFEnsembleModels(random_state=42)\n",
    "\n",
    "# Create base models\n",
    "base_models = ensemble.create_base_models()\n",
    "\n",
    "print(\"Base models created:\")\n",
    "for name in base_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit individual base models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING INDIVIDUAL BASE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    ensemble.fit_and_evaluate(\n",
    "        X_train, y_train, X_valid, y_valid,\n",
    "        preprocessor, name, model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING VOTING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "voting_model = ensemble.create_voting_ensemble(base_models)\n",
    "ensemble.fit_and_evaluate(\n",
    "    X_train, y_train, X_valid, y_valid,\n",
    "    preprocessor, 'voting', voting_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Ensemble\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING STACKING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stacking_model = ensemble.create_stacking_ensemble(base_models)\n",
    "ensemble.fit_and_evaluate(\n",
    "    X_train, y_train, X_valid, y_valid,\n",
    "    preprocessor, 'stacking', stacking_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "results_df = pd.DataFrame(ensemble.results).T\n",
    "results_df = results_df[['train_auc', 'valid_auc']].round(4)\n",
    "results_df['overfit'] = (results_df['train_auc'] - results_df['valid_auc']).round(4)\n",
    "results_df = results_df.sort_values('valid_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "print(\"\\nBest model:\", results_df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig = ensemble.plot_roc_curves(y_valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "fig = ensemble.plot_precision_recall_curves(y_valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis (for Causal Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "preprocessor_fitted = ensemble.models['rf'].named_steps['preprocess']\n",
    "\n",
    "# Numeric feature names (unchanged)\n",
    "num_feature_names = num_cols\n",
    "\n",
    "# Categorical feature names (one-hot encoded)\n",
    "cat_encoder = preprocessor_fitted.named_transformers_['cat'].named_steps['onehot']\n",
    "cat_feature_names = cat_encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "# All feature names\n",
    "all_feature_names = num_feature_names + list(cat_feature_names)\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Random Forest feature importance\n",
    "rf_importance = ensemble.analyze_feature_importance('rf', all_feature_names, top_n=30)\n",
    "\n",
    "print(\"\\nTop 30 Features (Random Forest):\")\n",
    "print(rf_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "from hef_ensemble_models import plot_feature_importance\n",
    "\n",
    "fig = plot_feature_importance(\n",
    "    rf_importance, \n",
    "    top_n=20, \n",
    "    title='Top 20 Most Important Features (Random Forest)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_name, best_model = ensemble.get_best_model()\n",
    "print(f\"Using {best_name} for Kaggle submission\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_proba = best_model.predict_proba(X_test_fe)[:, 1]\n",
    "\n",
    "print(f\"\\nTest predictions:\")\n",
    "print(f\"  Min: {test_proba.min():.4f}\")\n",
    "print(f\"  Max: {test_proba.max():.4f}\")\n",
    "print(f\"  Mean: {test_proba.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(test_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'HOSPITAL_EXPIRE_FLAG': test_proba\n",
    "})\n",
    "\n",
    "# Save to outputs\n",
    "output_path = \"/mnt/user-data/outputs/kaggle_submission.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to: {output_path}\")\n",
    "print(f\"Rows: {len(submission)}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Additional Ensemble: Weighted Average of Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted ensemble prediction\n",
    "weights = ensemble.create_final_ensemble_weights()\n",
    "\n",
    "print(\"Performance-based weights:\")\n",
    "for name, weight in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "    auc = ensemble.results[name]['valid_auc']\n",
    "    print(f\"  {name}: {weight:.3f} (AUC: {auc:.4f})\")\n",
    "\n",
    "# Weighted predictions on validation\n",
    "weighted_proba_valid = ensemble.predict_weighted_ensemble(X_valid, weights)\n",
    "weighted_auc = roc_auc_score(y_valid, weighted_proba_valid)\n",
    "\n",
    "print(f\"\\nWeighted Ensemble AUC on validation: {weighted_auc:.4f}\")\n",
    "\n",
    "# If better, use for Kaggle\n",
    "if weighted_auc > ensemble.results[best_name]['valid_auc']:\n",
    "    print(\"\\nâœ“ Weighted ensemble is better! Using for final submission.\")\n",
    "    test_proba_weighted = ensemble.predict_weighted_ensemble(X_test_fe, weights)\n",
    "    \n",
    "    submission_weighted = pd.DataFrame({\n",
    "        'HOSPITAL_EXPIRE_FLAG': test_proba_weighted\n",
    "    })\n",
    "    \n",
    "    output_path_weighted = \"/mnt/user-data/outputs/kaggle_submission_weighted.csv\"\n",
    "    submission_weighted.to_csv(output_path_weighted, index=False)\n",
    "    print(f\"Weighted submission saved to: {output_path_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Best Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best model\n",
    "model_path = \"/mnt/user-data/outputs/best_model.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"Best model ({best_name}) saved to: {model_path}\")\n",
    "\n",
    "# Also save feature importance\n",
    "importance_path = \"/mnt/user-data/outputs/feature_importance.csv\"\n",
    "rf_importance.to_csv(importance_path, index=False)\n",
    "print(f\"Feature importance saved to: {importance_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
