{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b532882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dir: c:\\Users\\sffra\\Downloads\\BSE 2025-2026\\cml_final\n",
      "Train path: c:\\Users\\sffra\\Downloads\\BSE 2025-2026\\cml_final\\data\\raw\\MIMIC III dataset HEF\\mimic_train_HEF.csv\n",
      "Test path: c:\\Users\\sffra\\Downloads\\BSE 2025-2026\\cml_final\\data\\raw\\MIMIC III dataset HEF\\mimic_test_HEF.csv\n",
      "Train shape: (20885, 44)\n",
      "Test shape: (5221, 39)\n",
      "Task: class (target = HOSPITAL_EXPIRE_FLAG)\n",
      "X_train_raw shape: (20885, 35)\n",
      "X_test_raw shape: (5221, 35)\n",
      "y_train shape: (20885,)\n",
      "Positive rate (death): 0.112\n",
      "[clean_min_bp_outliers] SysBP_Min: setting 99 values (0.530% of valid) below 40.0 mmHg to NaN\n",
      "[clean_min_bp_outliers] DiasBP_Min: setting 6 values (0.032% of valid) below 10.0 mmHg to NaN\n",
      "[clean_min_bp_outliers] MeanBP_Min: setting 797 values (4.262% of valid) below 30.0 mmHg to NaN\n",
      "[clean_min_bp_outliers] SysBP_Min: setting 31 values (0.664% of valid) below 40.0 mmHg to NaN\n",
      "[clean_min_bp_outliers] DiasBP_Min: setting 1 values (0.021% of valid) below 10.0 mmHg to NaN\n",
      "[clean_min_bp_outliers] MeanBP_Min: setting 227 values (4.857% of valid) below 30.0 mmHg to NaN\n",
      "Final X_train shape: (20885, 35)\n",
      "Final X_test shape: (5221, 35)\n",
      "(20885, 35) (20885,)\n",
      "Positive rate: 0.112\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Go from .../cml_final/notebooks/HEF -> .../cml_final\n",
    "BASE_DIR = Path.cwd().parents[1]\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "\n",
    "# Make sure Python can see the src folder\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from hef_prep import prepare_data\n",
    "from hef_prep import add_engineered_features  # and any other functions you want\n",
    "\n",
    "\n",
    "# Classification task, with feature engineering\n",
    "X, y, X_test = prepare_data(\n",
    "    task=\"class\",\n",
    "    leak_cols=[\"ADMITTIME\", \"ICD9_diagnosis\", \"DIAGNOSIS\", \"DOB\", \"DEATHTIME\", \"DISCHTIME\", \"DOD\", \"LOS\", \"HOSPITAL_EXPIRE_FLAG\"], \n",
    "    apply_fe=False,\n",
    ")\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(\"Positive rate:\", y.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0361b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 16708 Valid size: 4177\n",
      "Train positive rate: 0.112\n",
      "Valid positive rate: 0.112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Train/valid split (stratified)\n",
    "X_train, X_valid, y_train_split, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \"Valid size:\", X_valid.shape[0])\n",
    "print(\"Train positive rate:\", y_train_split.mean().round(3))\n",
    "print(\"Valid positive rate:\", y_valid.mean().round(3))\n",
    "\n",
    "# Feature type lists\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d92c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best CV AUC: 0.8096721827600112\n",
      "Best params:\n",
      "  clf__class_weight: balanced\n",
      "  clf__max_depth: None\n",
      "  clf__max_features: sqrt\n",
      "  clf__min_samples_leaf: 2\n",
      "  clf__min_samples_split: 7\n",
      "  clf__n_estimators: 203\n",
      "Random Forest (tuned) ROC-AUC on validation: 0.8181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Base RF (we'll tune around this)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", rf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Hyperparameter search space (tweaked)\n",
    "param_distributions = {\n",
    "    # Fewer trees → big runtime win, tiny performance loss (if any)\n",
    "    \"clf__n_estimators\": randint(150, 301),  # 150–300\n",
    "\n",
    "    # Try a tiny bit more variety in depth\n",
    "    \"clf__max_depth\": [10, 14, 18, None],    # add moderate & full depth\n",
    "\n",
    "    # Slightly wider and more standard ranges\n",
    "    \"clf__min_samples_split\": randint(2, 21),  # 2–20\n",
    "    \"clf__min_samples_leaf\":  randint(1, 11),  # 1–10\n",
    "\n",
    "    # Still just sqrt is fine; it's the usual RF default for classification\n",
    "    \"clf__max_features\": [\"sqrt\"],\n",
    "\n",
    "    # Worth considering class imbalance\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,       # can keep 20 if time is tight\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,            # 3-fold: ~40% faster than 5-fold\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train_split)\n",
    "\n",
    "\n",
    "rf_search.fit(X_train, y_train_split)\n",
    "\n",
    "print(\"Best CV AUC:\", rf_search.best_score_)\n",
    "print(\"Best params:\")\n",
    "for k, v in rf_search.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Evaluate on hold-out validation set\n",
    "best_rf_pipe = rf_search.best_estimator_\n",
    "y_valid_proba_rf = best_rf_pipe.predict_proba(X_valid)[:, 1]\n",
    "rf_auc = roc_auc_score(y_valid, y_valid_proba_rf)\n",
    "print(f\"Random Forest (tuned) ROC-AUC on validation: {rf_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc78592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy (stratified) ROC-AUC on validation: 0.4849\n",
      "Random Forest (tuned) ROC-AUC on validation: 0.8181\n",
      "AUC improvement over dummy: 0.3331\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Dummy baseline: predicts classes according to training distribution\n",
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "\n",
    "dummy_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),  # still impute/encode, even if dummy ignores X\n",
    "        (\"clf\", dummy),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_pipe.fit(X_train, y_train_split)\n",
    "y_valid_proba_dummy = dummy_pipe.predict_proba(X_valid)[:, 1]\n",
    "dummy_auc = roc_auc_score(y_valid, y_valid_proba_dummy)\n",
    "\n",
    "print(f\"Dummy (stratified) ROC-AUC on validation: {dummy_auc:.4f}\")\n",
    "print(f\"Random Forest (tuned) ROC-AUC on validation: {rf_auc:.4f}\")\n",
    "print(f\"AUC improvement over dummy: {rf_auc - dummy_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf4a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "def evaluate_probs_and_labels(name, y_true, y_proba, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(f\"ROC-AUC:  {auc:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision:{prec:.4f}\")\n",
    "    print(f\"Recall:   {rec:.4f}\")\n",
    "    print(f\"F1:       {f1:.4f}\")\n",
    "    print(\"Confusion matrix [ [TN FP], [FN TP] ]:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaac714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Always NEGATIVE (0) baseline\n",
      "----------------------------\n",
      "ROC-AUC:  0.5000\n",
      "Accuracy: 0.8877\n",
      "Precision:0.0000\n",
      "Recall:   0.0000\n",
      "F1:       0.0000\n",
      "Confusion matrix [ [TN FP], [FN TP] ]:\n",
      "[[3708    0]\n",
      " [ 469    0]]\n"
     ]
    }
   ],
   "source": [
    "dummy_neg = DummyClassifier(strategy=\"constant\", constant=0)\n",
    "\n",
    "dummy_neg_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", dummy_neg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_neg_pipe.fit(X_train, y_train_split)\n",
    "\n",
    "# Probabilities: model puts prob=0 for class 1 everywhere\n",
    "y_valid_proba_neg = dummy_neg_pipe.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred_neg  = dummy_neg_pipe.predict(X_valid)\n",
    "\n",
    "evaluate_probs_and_labels(\"Always NEGATIVE (0) baseline\", y_valid, y_valid_proba_neg, y_valid_pred_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81db74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Always POSITIVE (1) baseline\n",
      "----------------------------\n",
      "ROC-AUC:  0.5000\n",
      "Accuracy: 0.1123\n",
      "Precision:0.1123\n",
      "Recall:   1.0000\n",
      "F1:       0.2019\n",
      "Confusion matrix [ [TN FP], [FN TP] ]:\n",
      "[[   0 3708]\n",
      " [   0  469]]\n"
     ]
    }
   ],
   "source": [
    "dummy_pos = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "\n",
    "dummy_pos_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", dummy_pos),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_pos_pipe.fit(X_train, y_train_split)\n",
    "\n",
    "y_valid_proba_pos = dummy_pos_pipe.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred_pos  = dummy_pos_pipe.predict(X_valid)\n",
    "\n",
    "evaluate_probs_and_labels(\"Always POSITIVE (1) baseline\", y_valid, y_valid_proba_pos, y_valid_pred_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c78105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (tuned)\n",
      "---------------------\n",
      "ROC-AUC:  0.8181\n",
      "Accuracy: 0.9002\n",
      "Precision:0.7203\n",
      "Recall:   0.1812\n",
      "F1:       0.2896\n",
      "Confusion matrix [ [TN FP], [FN TP] ]:\n",
      "[[3675   33]\n",
      " [ 384   85]]\n"
     ]
    }
   ],
   "source": [
    "y_valid_proba_rf = best_rf_pipe.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred_rf  = best_rf_pipe.predict(X_valid)\n",
    "\n",
    "evaluate_probs_and_labels(\"Random Forest (not tuned)\", y_valid, y_valid_proba_rf, y_valid_pred_rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
